# -*- coding: utf-8 -*-
"""小港測站.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zgPDbiNwe7yMiosIcD5Gl-fR6vYV5JF_
"""

import os
import random
import numpy as np
import pandas as pd
from math import sqrt
from pandas import read_csv
from scipy.signal import medfilt
from matplotlib import pyplot as plt
from scipy.ndimage import gaussian_filter1d
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn import preprocessing
from sklearn import utils
from pandas import read_csv
from datetime import datetime
from numpy import concatenate
from sklearn.metrics import mean_squared_error
from keras.utils.vis_utils import plot_model
from keras.models import Sequential
from keras.layers import Dense , Dropout , Activation ,Flatten , Bidirectional# , Bidirectional , Bidirectional
from keras.layers import Conv1D
from keras.layers import MaxPooling1D
from keras.layers import Embedding
from keras.layers import LSTM, SimpleRNN
from keras.layers import TimeDistributed
from keras.layers import Bidirectional
from keras.layers import LeakyReLU
from keras.callbacks import EarlyStopping , ModelCheckpoint
import keras.backend as K
from keras import regularizers
from keras import optimizers
from keras.losses import mean_squared_error
import tensorflow as tf
from matplotlib import pyplot
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import r2_score
from keras.models import load_model

# dataset = read_csv(r'/content/2020_2021_data_0406.csv',encoding='utf-8')

dataset_pm25 = read_csv(r'/content/2018_2020_test.csv',encoding='utf-8')
dataset_pm10 = read_csv(r'/content/2018_2020_PM10.csv',encoding='utf-8')


# 將time設定為標籤
dataset_pm25 = dataset_pm25.set_index('time')
dataset_pm10 = dataset_pm10.set_index('time')

print(dataset_pm10)
print(dataset_pm25)

# 雜訊過濾器
filter_on = 1
if filter_on == 1:
  dataset_pm25['pm2.5']= medfilt(dataset_pm25['pm2.5'],3)
  dataset_pm25['pm2.5'] = gaussian_filter1d(dataset_pm25['pm2.5'],1)

print(dataset_pm25)

# 雜訊過濾器
filter_on = 1
if filter_on == 1:
  dataset_pm10['pm10']= medfilt(dataset_pm10['pm10'],3)
  dataset_pm10['pm10'] = gaussian_filter1d(dataset_pm10['pm10'],1)

print(dataset_pm10)

def normalize(train):
	train_norm = train.apply(lambda x:(x - np.min(x))/(np.max(x)- np.min(x)))
	return train_norm

dataset1_pm25 = normalize(dataset_pm25)

dataset1_pm10 = normalize(dataset_pm10)

dataset1_pm25['pre'] = dataset_pm25['pm2.5']
dataset1_pm10['pre'] = dataset_pm10['pm10']

dataset1_pm25.to_csv("midterm1_pm25.csv")
dataset1_pm10.to_csv("midterm1_pm10.csv")

# 特徵數目
feacture_number = 2

fname = os.path.join("/content/midterm1_pm10.csv")

f = open(fname)

data = f.read()

f.close()

# data的資料用換行符號\n進行分割
lines  = data.split('\n')

# 標題
header = lines[0].split(',')

# 去掉標題
# lines  = lines[1:17273]
lines  = lines[1:26480]


# 將資料轉換成numpy的形式
float_data_pm10 = np.zeros((len(lines),feacture_number))
float_data_new_pm10 = np.zeros((len(lines),1))

# 資料分割及存入陣列中
for i ,line in enumerate(lines):
    values = [float(x) for x in line.split(',')[1:]]
    float_data_pm10[i,:] = values[0:feacture_number]
    float_data_new_pm10[i,:] = values[feacture_number]

# 資料時序性分析
# data                   : 原始浮點數資料的陣列
# lookback               : 輸入資料應回朔多少個時間點 
# delay                  : 目標溫度應該在未來多少時間點
# min_index 和 max_index : 最小與最大索引值
# shuffle                : 是否按照時間順序來使用樣本或打亂樣本
# step                   : 產生樣本的時間的間隔

def generator(data1,data2, lookback, delay, min_index, max_index, shuffle=False, batch_size = 128, step=6 ):
    
    if max_index is None:
        max_index = len(data1)-delay-1
    i = min_index+lookback
    
    while 1:
        if shuffle :
            rows = np.random.randint(min_index+lookback,max_index,size=batch_size)
        else:
            if i+batch_size>=max_index:
                i = min_index+lookback
            rows = np.arange(i, min(i+batch_size,max_index))
            i += len(rows)
        samples = np.zeros((len(rows),lookback//step,data1.shape[-1]))
        targets = np.zeros((len(rows),))
        for j,row in enumerate(rows):
            indices = range(rows[j]-lookback,rows[j],step)
            samples[j]= data1[indices]
            targets[j]= data2[rows[j]+delay]
        yield samples,targets

lookback  = 25
batch_size = 150
step    = 1

# 訓練資料
train_data = generator(data1=float_data_pm10,data2=float_data_new_pm10,lookback=lookback,delay=1,min_index=0,max_index=21184,batch_size=batch_size,step=step)

# 驗證資料
valid_data = generator(data1=float_data_pm10,data2=float_data_new_pm10,lookback=lookback,delay=1,min_index=21185,max_index=26400,batch_size=batch_size,step=step)

# 測試資料
test_data  = generator(data1=float_data_pm10,data2=float_data_new_pm10,lookback=lookback,delay=1,min_index=21022,max_index=None,batch_size=4500,step=step)

# valid_data產生器需要運行多少次才可以產生完整的驗證集
# val_steps = (12500-11001-lookback)//batch_size
val_steps = (26400-21185-lookback)//batch_size

# test_data產生器需要運行多少次才可以產生完整的測試集
test_steps = (len(float_data_pm10)-7679)//batch_size

model = Sequential()
# model.add(Conv1D(input_shape=(lookback//step,float_data.shape[-1]),filters=128,kernel_size=3,padding='same',activation='relu',kernel_initializer="glorot_uniform",kernel_regularizer=regularizers.l1(0.001)))
# model.add(Conv1D(filters=128,kernel_size=3,padding='same',activation='relu',kernel_initializer="glorot_uniform",kernel_regularizer=regularizers.l1(0.001)))
# model.add(MaxPooling1D(pool_size=2,padding='same'))

# model.add(LSTM(256,activation='relu',return_sequences=True,input_shape=(lookback//step,float_data.shape[-1]),kernel_regularizer=regularizers.l1(0.001),kernel_initializer="glorot_uniform"))
model.add(LSTM(256,activation='relu',return_sequences=True,input_shape=(lookback//step,float_data_pm10.shape[-1]),kernel_regularizer=regularizers.l1(0.001),kernel_initializer="glorot_uniform",recurrent_activation='sigmoid'))


model.add(Flatten())
# model.add(Dropout(0.4 , noise_shape=None , seed=None))
# model.add(Dense(256 , activation ='relu',kernel_initializer="glorot_uniform"))
model.add(Dense(256 , activation ='relu',kernel_initializer="glorot_uniform"))

# model.add(Dropout(0.2 , noise_shape=None , seed=None))
model.add(Dense(1 , activation ='linear',kernel_initializer="glorot_uniform"))

adam=tf.optimizers.Adam(0.01)

model.compile(loss ='mae' , optimizer=adam, metrics=['mse'])

# callback = EarlyStopping(monitor="loss" , patience =20, verbose =1, mode="auto")

history = model.fit_generator(train_data,steps_per_epoch=46,epochs=300,validation_data=valid_data,validation_steps=val_steps)

model.summary()

model.save('my_model_pm10.h5')

# 將儲存的檔案存入
model = load_model('my_model_pm10.h5')

test_value_pm10 = next(test_data)

pre_data = model.predict(test_value_pm10[0])



pre_pm10=[]
true_pm10=[]

for i in range(len(test_value_pm10[0])):
  pre_pm10.append(pre_data[i][0])
  true_pm10.append(test_value_pm10[1][i])

print(len(pre_pm10))
print(len(true_pm10))

pyplot.figure(figsize=(18,10)) 
plt.title('TrueValue and PreValue ')
pyplot.plot(true_pm10, label='TRUE')
pyplot.plot(pre_pm10, label='PRE')
pyplot.legend()
pyplot.show()

mse = metrics.mean_squared_error(true_pm10,pre_pm10)
rmse = metrics.mean_squared_error(true_pm10,pre_pm10)**0.5
mae = metrics.mean_absolute_error(true_pm10,pre_pm10)
mape = (metrics.mean_absolute_percentage_error(true_pm10,pre_pm10))*100
r2   = r2_score(true_pm10,pre_pm10)
print('mse :',mse)
print('rmse :',rmse)
print('mae :',mae)
print('mape :',mape)
print('r2 :',r2)

fname = os.path.join("/content/midterm1_pm25.csv")

f = open(fname)

data = f.read()

f.close()

# data的資料用換行符號\n進行分割
lines  = data.split('\n')

# 標題
header = lines[0].split(',')

# 去掉標題
# lines  = lines[1:17273]
lines  = lines[1:26030]


# 將資料轉換成numpy的形式
float_data_pm25 = np.zeros((len(lines),feacture_number))
float_data_new_pm25 = np.zeros((len(lines),1))

# 資料分割及存入陣列中
for i ,line in enumerate(lines):
    values = [float(x) for x in line.split(',')[1:]]
    float_data_pm25[i,:] = values[0:feacture_number]
    float_data_new_pm25[i,:] = values[feacture_number]

lookback  = 25
batch_size = 150
step    = 1

# 訓練資料
# train_data = generator(data1=float_data,data2=float_data_new,lookback=lookback,delay=9,min_index=0,max_index=8684,batch_size=batch_size,step=step)
# train_data = generator(data1=float_data,data2=float_data_new,lookback=lookback,delay=8,min_index=0,max_index=12000,batch_size=batch_size,step=step)
train_data = generator(data1=float_data_pm25,data2=float_data_new_pm25,lookback=lookback,delay=1,min_index=0,max_index=17444,batch_size=batch_size,step=step)

# 驗證資料
# valid_data = generator(data1=float_data,data2=float_data_new,lookback=lookback,delay=9,min_index=8685,max_index=11000,batch_size=batch_size,step=step)
# valid_data = generator(data1=float_data,data2=float_data_new,lookback=lookback,delay=8,min_index=12001,max_index=13500,batch_size=batch_size,step=step)
valid_data = generator(data1=float_data_pm25,data2=float_data_new_pm25,lookback=lookback,delay=1,min_index=17445,max_index=21021,batch_size=batch_size,step=step)

# 測試資料
# test_data  = generator(data1=float_data,data2=float_data_new,lookback=lookback,delay=9,min_index=11001,max_index=None,batch_size=6000,step=step)
# test_data  = generator(data1=float_data,data2=float_data_new,lookback=lookback,delay=8,min_index=13501,max_index=None,batch_size=3600,step=step)
test_data  = generator(data1=float_data_pm25,data2=float_data_new_pm25,lookback=lookback,delay=1,min_index=21022,max_index=None,batch_size=4500,step=step)

# valid_data產生器需要運行多少次才可以產生完整的驗證集
# val_steps = (12500-11001-lookback)//batch_size
val_steps = (21021-17445-lookback)//batch_size

# test_data產生器需要運行多少次才可以產生完整的測試集
test_steps = (len(float_data_pm25)-7679)//batch_size


# print(len(float_data))

model = Sequential()
# model.add(Conv1D(input_shape=(lookback//step,float_data.shape[-1]),filters=128,kernel_size=3,padding='same',activation='relu',kernel_initializer="glorot_uniform",kernel_regularizer=regularizers.l1(0.001)))
# model.add(Conv1D(filters=128,kernel_size=3,padding='same',activation='relu',kernel_initializer="glorot_uniform",kernel_regularizer=regularizers.l1(0.001)))
# model.add(MaxPooling1D(pool_size=2,padding='same'))

# model.add(LSTM(256,activation='relu',return_sequences=True,input_shape=(lookback//step,float_data.shape[-1]),kernel_regularizer=regularizers.l1(0.001),kernel_initializer="glorot_uniform"))
model.add(LSTM(256,activation='relu',return_sequences=True,input_shape=(lookback//step,float_data_pm25.shape[-1]),kernel_regularizer=regularizers.l1(0.001),kernel_initializer="glorot_uniform",recurrent_activation='sigmoid'))


model.add(Flatten())
# model.add(Dropout(0.4 , noise_shape=None , seed=None))
# model.add(Dense(256 , activation ='relu',kernel_initializer="glorot_uniform"))
model.add(Dense(256 , activation ='relu',kernel_initializer="glorot_uniform"))

# model.add(Dropout(0.2 , noise_shape=None , seed=None))
model.add(Dense(1 , activation ='linear',kernel_initializer="glorot_uniform"))

adam=tf.optimizers.Adam(0.01)

model.compile(loss ='mae' , optimizer=adam, metrics=['mse'])

# callback = EarlyStopping(monitor="loss" , patience =20, verbose =1, mode="auto")

history = model.fit_generator(train_data,steps_per_epoch=46,epochs=300,validation_data=valid_data,validation_steps=val_steps)

model.summary()

model.save('my_model_pm25.h5')

# 將儲存的檔案存入
model = load_model('my_model_pm25.h5')

test_value_pm25 = next(test_data)

pre_data = model.predict(test_value_pm25[0])



pre_pm25=[]
true_pm25=[]

for i in range(len(test_value_pm25[0])):
  pre_pm25.append(pre_data[i][0])
  true_pm25.append(test_value_pm25[1][i])

print(len(pre_pm25))
print(len(true_pm25))

pyplot.figure(figsize=(18,10)) 
plt.title('TrueValue and PreValue ')
pyplot.plot(true_pm25, label='TRUE')
pyplot.plot(pre_pm25, label='PRE')
pyplot.legend()
pyplot.show()

mse = metrics.mean_squared_error(true_pm25,pre_pm25)
rmse = metrics.mean_squared_error(true_pm25,pre_pm25)**0.5
mae = metrics.mean_absolute_error(true_pm25,pre_pm25)
mape = (metrics.mean_absolute_percentage_error(true_pm25,pre_pm25))*100
r2   = r2_score(true_pm25,pre_pm25)
print('mse :',mse)
print('rmse :',rmse)
print('mae :',mae)
print('mape :',mape)
print('r2 :',r2)